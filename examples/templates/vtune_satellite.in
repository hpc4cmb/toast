#!/bin/bash

#SBATCH --partition=@QUEUE@
@CONSTRAINT@
#SBATCH --account=@ACCOUNT@
#SBATCH --nodes=@NODES@
#SBATCH --time=@TIME@
#SBATCH --job-name=@SIZE@-satellite
#SBATCH --output=out_vtune_@SIZE@_satellite_@MACHINE@_%j.log
#SBATCH --perf=@VTUNE@

set -o errexit

@EXTRA_INIT@

echo Starting slurm script at $(date)

ulimit -s unlimited

echo -e "\n-----------------------------------------------------------------------"
echo -e "ENVIRONMENT:\n"
env
echo -e "-----------------------------------------------------------------------\n"
echo "PYTHON: $(which python)"
echo "PYTHON VERSION: $(python --version &> /dev/stdout)"
echo ""

pstr=@MACHINE@
outdir="out_vtune_@SIZE@_satellite_${pstr}"
mkdir -p "${outdir}"

# This script assumes that you are running at NERSC and have already
# loaded the toast module for the correct machine / configuration.

# This should be the same as the --nodes option above
nodes=@NODES@

# How many processes are we running per node?  Handle
# the case of a very small simulation.
if [ @NOBS@ -lt 50 ]; then
    node_proc=1
else
    node_proc=@NODEPROCS@
fi

# Generate the focalplane file if it does not already exist.

detpix=@DETPIX@

fpfile="${outdir}/fp_${detpix}.pkl"
if [ ! -e "${fpfile}" ]; then
    echo "Running toast_fake_focalplane.py..."
    srun -n 1 -N 1 toast_fake_focalplane.py \
        --disable-timer-serialization \
        --toast-output-dir ${outdir} --toast-timing-fname "timing_report_fp" \
        --minpix ${detpix} --out "${outdir}/fp"
fi

# The executable script

ex=$(which toast_satellite_sim.py)
echo "Using ${ex}"

# Scan strategy parameters from a file

parfile="@TOPDIR@/params/satellite/sim_noise_hwp.par"

# Observations

nobs=@NOBS@

groupnodes=@GROUPNODES@

# Data distribution parameters.  We are distributing by detector,
# so if our number of processes in a group is larger than the number
# of detectors this is bad.  In that case, set the group size to 
# one, so we have many more groups, each assigned

if [ ${node_proc} -gt ${detpix} ]; then
    groupsize=1
else
    groupsize=$(( node_proc * groupnodes ))
fi

# The commandline

com="${ex} @${parfile} \
--toast-output-dir ${outdir} \
--enable-timers --enable-timer-serialization \
--groupsize ${groupsize} \
--fp ${fpfile} \
--numobs ${nobs} \
--outdir ${outdir}/out \
$@ \
"

#--- Hardware configuration ----

# Hyperthread CPUs per physical core
cpu_per_core=@HYPERTHREAD@

# Physical cores we are using
node_cores=@NODECORES@

node_thread=$(( node_cores / node_proc ))
node_depth=$(( cpu_per_core * node_thread ))
procs=$(( nodes * node_proc ))

export OMP_NUM_THREADS=${node_thread}
export OMP_PLACES=threads
export OMP_PROC_BIND=spread

# Set TMPDIR to be on the ramdisk
export TMPDIR=/dev/shm

run="srun --cpu_bind=cores -n ${procs} -N ${nodes} -c ${node_depth}"

# only set "vrun" if not defined in environment (see vtune_params/ directory)
: ${vrun:="amplxe-cl -collect hotspots -knob sampling-interval=50 \
    -run-pass-thru=--no-altstack -r ${outdir}/vtune \
    -finalization-mode=deferred -trace-mpi"}
: ${vtune_collect:="pyhotspots"}

export vrun
export vtune_collect

echo Calling srun at $(date)

: ${LOG_OUT:="${outdir}/log_${vtune_collect}"}
echo "${run} ${vrun} -r ${outdir}/vtune_${vtune_collect}  -- ${com}"
eval ${run} ${vrun} -r ${outdir}/vtune_${vtune_collect}  -- ${com} > ${LOG_OUT} 2>&1

if eval command -v vfinal &> /dev/null ; then
    echo "Run \"vfinal <vtune-output-dir>\" to finalize results"
else
    echo "No alias \"vfinal\" defined. Remember to finalize results"
fi

echo End slurm script at $(date)

