#!/bin/bash

#SBATCH --partition=@QUEUE@
@CONSTRAINT@
#SBATCH --account=@ACCOUNT@
#SBATCH --nodes=@NODES@
#SBATCH --time=@TIME@
#SBATCH --job-name=@SIZE@-ground_multisite
#SBATCH --output=out_@SIZE@_ground_multisite_@MACHINE@_%j.log
#DW jobdw capacity=@BUFFER_SIZE@ access_mode=striped type=scratch pool=@BUFFER_POOL@

: ${DW_JOB_STRIPED:=/tmp}
TOAST_TMPDIR_RET=1
export TOAST_TMPDIR=$(mktemp -d ${DW_JOB_STRIPED}/toast-XXXXXX && TOAST_TMPDIR_RET=0)
if [ "${TOAST_TMPDIR_RET}" -gt 0 ]; then
    export TOAST_TMPDIR=$(mktemp -d ${PWD}/toast-XXXXXX)
fi
echo "TOAST_TMPDIR: ${TOAST_TMPDIR}"

set -o errexit
set -v

@EXTRA_INIT@

echo Starting slurm script at $(date)

export PYTHONNOUSERSITE=1
export HOME=${SCRATCH}

echo -e "\n-----------------------------------------------------------------------"
echo -e "ENVIRONMENT:\n"
env | sort -d
echo -e "-----------------------------------------------------------------------\n"
echo "PYTHON: $(which python)"
echo "PYTHON VERSION: $(python --version &> /dev/stdout)"
echo ""

pstr=@MACHINE@
outdir="out_@SIZE@_ground_multisite_${pstr}"
mkdir -p "${outdir}"

# This script assumes that you are running at NERSC and have already
# loaded the toast module for the correct machine / configuration.

# This should be the same as the --nodes option above
nodes=@NODES@

# How many processes are we running per node?  Handle
# the case of a very small simulation.
if [ "x@TINY@" = "xyes" ]; then
    node_proc=1
else
    node_proc=@NODEPROCS@
fi

# Generate the focalplane files if they do not already exist.

detpix=@DETPIX@

fpparfile="@TOPDIR@/params/ground/focalplane.par"

fpfile_atacama="${outdir}/fp_atacama_${detpix}.pkl"
if [ ! -e "${fpfile_atacama}" ]; then
    echo "Running toast_fake_focalplane.py..."
    srun -n 1 -N 1 toast_fake_focalplane.py @$fpparfile \
         --disable-timer-serialization \
         --toast-output-dir "${outdir}" --toast-timing-fname "timing_report_fp" \
         --out "${outdir}/fp_atacama" --minpix $detpix --fov 1
fi

fpfile_pole="${outdir}/fp_pole_${detpix}.pkl"
if [ ! -e "${fpfile_pole}" ]; then
    echo "Running toast_fake_focalplane.py (pole)..."
    srun -n 1 -N 1 toast_fake_focalplane.py @$fpparfile \
         --disable-timer-serialization \
         --toast-output-dir "${outdir}" --toast-timing-fname "timing_report_fp_pole" \
         --out "${outdir}/fp_pole" --minpix $detpix --fov 1
fi

# Generate the schedule files if they do not already exist.

operational_days=$(( @NODES@ / @NODES_PER_GROUP@ ))

schparfile="@TOPDIR@/params/ground/schedule.par"
schedulefile_atacama="${outdir}/schedule_atacama.txt"
if [ ! -e "${schedulefile_atacama}" ]; then
    echo "Running toast_ground_schedule.py (atacama)..."
    srun -n 1 -N 1 toast_ground_schedule.py @$schparfile \
         --disable-timer-serialization \
         --toast-output-dir "${outdir}" --toast-timing-fname "timing_report_ground_schedule_actcama" \
         --out "${schedulefile_atacama}" --patch @PATCH@ \
         --operational_days $operational_days
fi

schparfile="@TOPDIR@/params/ground/schedule_pole.par"
schedulefile_pole="${outdir}/schedule_pole.txt"
if [ ! -e "${schedulefile_pole}" ]; then
    echo "Running toast_ground_schedule.py (pole)..."
    srun -n 1 -N 1 toast_ground_schedule.py @$schparfile \
         --disable-timer-serialization \
         --toast-output-dir "${outdir}" --toast-timing-fname "timing_report_ground_schedule_pole" \
         --out "${schedulefile_pole}" --patch @PATCH@ \
         --operational_days $operational_days
fi

# The executable script

ex=$(which toast_ground_sim.py)
echo "Using ${ex}"

# Scan strategy parameters from a file

parfile="@TOPDIR@/params/ground/ground_sim_multisite.par"

# Data distribution parameters

groupsize=$(( @NODES_PER_GROUP@ * node_proc ))

# The commandline

com="${ex} @${parfile} \
--toast-output-dir "${outdir}" \
--toast-timing-fname "timing_report_main" \
--enable-timer-serialization \
--groupsize ${groupsize} \
--fp ${fpfile_atacama},${fpfile_pole} \
--schedule ${schedulefile_atacama},${schedulefile_pole} \
--out ${outdir}/out \
--atm_cache ${outdir}/atm_cache \
--atm_xstep @CELLSIZE@ \
--atm_ystep @CELLSIZE@ \
--atm_zstep @CELLSIZE@ \
--madam_iter_max @ITERMAX@ \
--samplerate @SAMPLERATE@ \
--scanrate @SCANRATE@ \
$@ \
"

#--- Hardware configuration (no need to change) ----

# Hyperthread CPUs per physical core
cpu_per_core=@HYPERTHREAD@

# Physical cores we are using
node_cores=@NODECORES@

node_thread=$(( node_cores / node_proc ))
node_depth=$(( cpu_per_core * node_thread ))
procs=$(( nodes * node_proc ))

export OMP_NUM_THREADS=${node_thread}
export OMP_PLACES=threads
export OMP_PROC_BIND=spread
export TOAST_NODE_COUNT=${nodes}
export TOAST_NUM_THREADS=${OMP_NUM_THREADS}
echo "OpenMP # threads: ${OMP_NUM_THREADS}"
echo "TOAST # threads: ${TOAST_NUM_THREADS}"

# Set TMPDIR to be on the ramdisk
export TMPDIR=/dev/shm

run="srun --cpu_bind=cores -n ${procs} -N ${nodes} -c ${node_depth}"

echo Calling srun at $(date)

: ${LOG_OUT:="${outdir}/log"}
echo "${run} ${com}"
eval ${run} ${com} > ${LOG_OUT} 2>&1

# if the glob is unsuccessful, don't pass ${outdir}/timing_report*.out
shopt -s nullglob
if [ -x ${PWD}/generate_notes.sh ]; then
    ${PWD}/generate_notes.sh ${outdir}
else
    for i in ${outdir}/timing_report*.out
    do
        echo -e "\n--> ${i}\n" >> ${LOG_OUT}
        head -n 1000 ${i} >> ${LOG_OUT}
    done
fi

echo End slurm script at $(date)
