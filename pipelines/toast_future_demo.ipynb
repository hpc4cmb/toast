{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import astropy.units as u\n",
    "\n",
    "import toast\n",
    "\n",
    "from toast import qarray as qa\n",
    "\n",
    "from toast import config as tc\n",
    "\n",
    "from toast import (\n",
    "    Telescope, \n",
    "    Focalplane, \n",
    "    Observation, \n",
    ")\n",
    "\n",
    "import toast.future_ops as ops\n",
    "\n",
    "from toast.future_ops.sim_focalplane import fake_hexagon_focalplane\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model\n",
    "\n",
    "The basic data model is a set of `Observation` instances, each of which is associated with a `Focalplane` on a `Telescope`.  Note that a Focalplane instance is probably just a sub-set of detectors on the actual physical focalplane.  These detectors must be co-sampled and likely have other things in common (for example, they are on the same wafer or are correlated in some other way).  For this example, we will manually create these objects, but usually these will be loaded / created by some experiment-specific function.\n",
    "\n",
    "MPI is optional in TOAST, although it is required to achieve good parallel performance on traditional CPU systems.  In this section we show how interactive use of TOAST can be done without any reference to MPI.  In a later section we show how to make use of distributed data and operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Focalplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Telescope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by making a fake focalplane\n",
    "\n",
    "focalplane_pixels = 7 # (hexagonal)\n",
    "field_of_view = 10.0 # degrees\n",
    "sample_rate = 10.0 # Hz\n",
    "\n",
    "focalplane = fake_hexagon_focalplane(\n",
    "    focalplane_pixels,\n",
    "    field_of_view,\n",
    "    samplerate=10.0,\n",
    "    epsilon=0.0,\n",
    "    net=1.0,\n",
    "    fmin=1.0e-5,\n",
    "    alpha=1.0,\n",
    "    fknee=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make a fake telescope\n",
    "\n",
    "telescope = Telescope(name=\"fake\", focalplane=focalplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty observation\n",
    "\n",
    "samples = 10\n",
    "\n",
    "obs = Observation(telescope, name=\"2020-07-31_A\", samples=samples)\n",
    "\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "By default, the observation is empty.  You can add arbitrary metadata to the observation- it acts just like a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = {\n",
    "    \"Temperature 1\": np.array([1.0, 2.0, 3.0]),\n",
    "    \"Other Sensor\": 1.2345\n",
    "}\n",
    "\n",
    "obs[\"housekeeping\"] = hk\n",
    "\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Ordered Data\n",
    "\n",
    "Now we can add some Time Ordered Data to this observation.  There are basically two types of data:  timestreams of information that all detectors have in common (telescope boresight, etc) and timestreams of detector data (signals and flags).  Although an Observation acts like a dictionary that can hold arbitrary keys, there are some standard built-in names for TOD quantities that are used by the Operator classes.  You can also create other custom types of data.  To see the built-in names, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.keynames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These underlying names can be overridden at construction time if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Ordered Detector Data\n",
    "\n",
    "Detector data has some unique properties that we often want to leverage in our analyses.  Each process has some detectors and some time slice of the observation.  In the case of a single process like this example, all the data is local.  Before using data we need to create it within the empty `Observation`.  Here we create the default SIGNAL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.create_signal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This special `DetectorData` class is a table that can be indexed either by name or by index.  You can set and get values as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.signal[\"D0A\"] = np.arange(samples, dtype=np.float64)\n",
    "\n",
    "obs.signal[1] = 10.0 * np.arange(samples, dtype=np.float64)\n",
    "\n",
    "print(obs.signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.signal[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.signal[\"D0A\", \"D0B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.signal[1][1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This showed the creation of the default \"SIGNAL\" detector data, but you can create other types of data.  For example, lets say you wanted to create some detector pointing matrix values consisting of a 64bit integer pixel number and three 32bit floats for the I/Q/U weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.create_detector_data(\"pixels\", shape=(samples,), dtype=np.int64)\n",
    "obs.create_detector_data(\"weights\", shape=(samples, 3), dtype=np.float32)\n",
    "\n",
    "print(obs[\"weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = obs[\"weights\"]\n",
    "\n",
    "for d in obs.detectors:\n",
    "    for s in range(samples):\n",
    "        weights[d][s] = [1.0, 0.5, 0.5]\n",
    "        \n",
    "print(obs[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Ordered Data Shared by all Detectors\n",
    "\n",
    "There are some types of timestreams which all detectors have in common within the observation.  These include things like telescope pointing, timestamps, and other quantities.  We want all processes to have access to these quantities.  However, this type of data is usually stored once and then read many times.  We use shared memory on the node to store this data to avoid duplicating it for every process.  For this simple serial example, the details are not important.  The main thing is to use a special method when creating these buffers in the observation.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.create_times()\n",
    "obs.create_boresight_radec()\n",
    "obs.create_common_flags()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accesses the timestamps regardless of the underlying\n",
    "# dictionary key and checks that the underlying buffer has\n",
    "# the right dimensions.\n",
    "\n",
    "print(obs.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.times[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_*()` methods create these shared memory objects of the correct default dimensions and type.  You can also create completely custom timestream data (see advanced topics below).\n",
    "\n",
    "After creating the shared buffer, we used the observation method `times()` to return the timestamps.  There are similar methods for all the \"standard\" observation data products (boresight_radec(), signal(), etc).  The benefit to using these methods instead of accessing the internal dictionary key directly is that there are checks on the shapes of the underlying objects to ensure consistency.  Also, an operator does not have to know the name of the underlying dictionary key, which might be different between experiments.\n",
    "\n",
    "These shared data objects have a `set()` method used to write to them.  This is more important when using MPI.  In the serial case, you can just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.times.set(np.arange(samples, dtype=np.float64))\n",
    "\n",
    "print(obs.times[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The `Observation` instances discussed previously are usually stored as a list inside a top-level container class called `Data`.  This class also stores the TOAST MPI communicator information.  For this serial example you can just instantiate an empty `Data` class and add things to the observation list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data()\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(data.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this `Data` object has no observations yet.  We'll fix that in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Model\n",
    "\n",
    "The `Operator` class defines the interfaces for operators working on data.  Each operator constructor takes only keyword arguments, and these keyword arguments are stored as class attributes in the instance.  Each operator has methods that describe the observation dictionary keys it requires for input and which keys it provides as output.  An operator has an `exec()` method that works with `Data` objects.  We will start by looking at the `SimSatellite` operator to simulate fake telescope scan strategies for a generic satellite.  We can always see the options and default values by using the standard help function or the '?' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ops.SimSatellite)\n",
    "\n",
    "?ops.SimSatellite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instantiate a class directly by overriding some defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsat = ops.SimSatellite(\n",
    "    n_observation=2, \n",
    "    observation_time=(5 * u.minute),\n",
    ")\n",
    "\n",
    "print(simsat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the operator is constructed, the parameters can be changed directly.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsat.telescope = telescope\n",
    "simsat.n_observation = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simsat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have an `Operator` that is ready to use.  This particular operator creates observations from scratch with telescope properties generated and stored.  We can create an empty `Data` object and then run this operator on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsat.exec(data)\n",
    "simsat.finalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} observations\".format(len(data.obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ob in data.obs:\n",
    "    print(ob.times[:5])\n",
    "    print(ob.boresight_radec[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that our `SimSatellite` operator has created just one observation of 360 samples in the `Data` object.  We can feed this tiny dataset to further operators to simulate signals or process the data.  Let's now simulate some noise timestreams for our detectors.  First we need to create a \"noise model\" for our detectors.  We can bootstrap this process by making a noise model from the nominal detector properties in the focalplane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.DefaultNoiseModel.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model_config = ops.DefaultNoiseModel.defaults()\n",
    "print(noise_model_config)\n",
    "\n",
    "noise_model = ops.DefaultNoiseModel(noise_model_config)\n",
    "noise_model.exec(data)\n",
    "noise_model.finalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use the `SimNoise` operator to simulate some timestreams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.SimNoise.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can just use all the default options.  It assumes the default noise model and if we don't specify the `out` key this operator just accumulates to the default detector data (\"SIGNAL\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_config = ops.SimNoise.defaults()\n",
    "print(noise_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the operator\n",
    "\n",
    "sim_noise = ops.SimNoise(noise_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it on the data\n",
    "\n",
    "sim_noise.exec(data)\n",
    "sim_noise.finalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the observation now has some signal.  Let's look at that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.obs[0].signal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just look at few samples from one detector in the first observation\n",
    "\n",
    "print(data.obs[0].signal[\"D1A\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Operator\n",
    "\n",
    "There is a special Operator class called `Pipeline` which serves as a way to group other operators together and run them in sequence (possibly running them on only a few detectors at a time).  The default is to run the list of operators on the full `Data` object in one shot.  The Pipeline class has the usual way of getting the defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.Pipeline.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see more about this Operator below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Files\n",
    "\n",
    "We saw above how operators are constructed with a dictionary of parameters.  **You can do everything by passing parameters when constructing operators**.  Configuration files are completely optional, but they do allow easy sharing of complicated pipeline setups.\n",
    "\n",
    "These parameters can be loaded from one or more files and used to automatically construct operators for use.  When doing this, each instance of an operator is given a \"name\" that can be used to reference it later.  This way you can have multiple operators of the same class doing different things within your pipeline.  If you have a script where you know which operators you are going to be using, you can get the defaults for the whole list at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ops = {\n",
    "    \"sim_satellite\": ops.SimSatellite,\n",
    "    \"noise_model\": ops.DefaultNoiseModel,\n",
    "    \"sim_noise\": ops.SimNoise,\n",
    "    \"pointing\": ops.PointingHealpix\n",
    "}\n",
    "\n",
    "conf = default_config(operators=pipe_ops)\n",
    "\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dump this to a file and look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = tempfile.mkdtemp()\n",
    "conf_file = os.path.join(tmpdir, \"test.toml\")\n",
    "\n",
    "dump_config(conf_file, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !cat {conf_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we can also load it back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newconf = load_config(conf_file)\n",
    "\n",
    "print(newconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to add a Pipeline to this configuration that reference the names of the two operators to use?  We can do that using a special syntax which consists of `@config:` followed by a UNIX-style path to the object we are referencing.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default Pipeline config\n",
    "\n",
    "sim_pipe_config = ops.Pipeline.defaults()\n",
    "print(sim_pipe_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add references to the operators\n",
    "\n",
    "sim_pipe_config[\"operators\"] = [\n",
    "    \"@config:/operators/sim_satellite\",\n",
    "    \"@config:/operators/noise_model\",\n",
    "    \"@config:/operators/sim_noise\",\n",
    "    \"@config:/operators/pointing\",\n",
    "]\n",
    "\n",
    "# Add the pipeline config to the main config\n",
    "\n",
    "newconf[\"operators\"][\"sim_pipe\"] = sim_pipe_config\n",
    "\n",
    "print(newconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could dump this to a file for later use.  What if we wanted to go and create operators from this?  We could loop through each key in the \"operators\" dictionary and instantiate the class with the config values.  However, there is a helper function that does this.  Before doing that we need to add a Telescope to this config for the satellite simulation.  Normally this would be done by some experiment-specific script that would create a more custom telescope / focalplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newconf[\"operators\"][\"sim_satellite\"][\"telescope\"] = telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate all the operators in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = create(newconf)\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the result looks similar, look more closely.  Our dictionary of configuration options is now actually a dictionary of instantiated classes.  We can now run these operators directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data()\n",
    "\n",
    "# Run the Pipeline operator, which in turn runs 2 other operators\n",
    "\n",
    "run[\"operators\"][\"sim_pipe\"].exec(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 samples of one detector in the first observation.\n",
    "\n",
    "print(data.obs[0].signal[\"D0A\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.obs[0][\"weights\"][\"D0A\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics\n",
    "\n",
    "The previous sections covered the `Observation` container and its interfaces, and how to create and run Operators on a `Data` object containing a list of observations.  The new data model has some aspects that improve our situation on larger runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Use\n",
    "\n",
    "Earlier we saw how the MPI shared memory objects created in an Observation are used to store things that are common to all detectors (boresight pointing, telescope velocity, etc).  These quantities have defaults for the shape, dtype, **and** the communicator used.  In the case of these common properties, the \"grid column communicator\" is used.  This includes all processes in the observation that share a common slice of time.  The result is that only one copy of these common data objects exist on each node, regardless of how many processes are running on the node.\n",
    "\n",
    "However, we can create completely custom shared memory objects.  Imagine that every single process needed some common telescope data to be able to work with its local signal.  We could create a shared object on the group communicator used for the whole observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "\n",
    "obs = Observation(telescope, name=\"2020-07-31_A\", samples=samples)\n",
    "\n",
    "# This is the same for every process, regardless of location in the process grid\n",
    "\n",
    "obs.create_shared_data(\n",
    "    \"same_for_every_process\", \n",
    "    shape=(100, 100), \n",
    "    dtype=np.float64, \n",
    "    comm=obs.mpicomm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another example, suppose that we had some detector-specific quantities (beams, bandpasses, etc) shared by all processes with data from a given detector.  We can store that in shared memory using the \"grid row communicator\" of the process grid, so that we only have one copy of those products per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same for every process along a row of the grid\n",
    "# so these processes all have the same detectors.\n",
    "\n",
    "obs.create_shared_data(\n",
    "    \"detector_aux_data\", \n",
    "    shape=(len(obs.local_detectors), 100), \n",
    "    dtype=np.float64, \n",
    "    comm=obs.grid_comm_row\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of MPI shared memory should greatly reduce our memory footprint when running many MPI processes per node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Subsets of Detectors\n",
    "\n",
    "The `Pipeline` operator is used to chain other operators together and can internally feed data to those sub-operators in sets of detectors.  This is a work in progress, but the workflow code that creates the `Pipeline` will be able to specify sets of detectors to process at once.  This set will be different on different processes.  The special strings \"ALL\" and \"SINGLE\" are used to either work with all detectors in one shot (the current toast default) or to loop over detectors individually, running all operators on those before moving on to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerator Use\n",
    "\n",
    "This covers planned features...\n",
    "\n",
    "For each supported architecture, if all operators in a pipeline support that hardware then the pipeline can use observation methods to copy select data to the accelerator at the beginning and back from the accelerator at the end.  Operators individually have methods that specify the observation keys they \"require\" and also the observation keys they \"provide\".  This allows logic in the pipeline operator to determine which intermediate data products of the operators are only on the accelerator and do not need to be moved back to the host system.  A Pipeline running on an accelerator will likely process only a few detectors at a time due to memory constraints.\n",
    "\n",
    "Observations already make use of MPI shared memory that is replicated across nodes.  Each node will have some number of accelerators.  We can assign each process to a particular accelerator and compute the minimal set of shared memory objects that need to be staged to each accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
