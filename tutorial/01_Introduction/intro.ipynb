{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This lesson is a brief introduction to TOAST and its data representations.  This next cell is just initializing some things for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load common tools for all lessons\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from lesson_tools import (\n",
    "    fake_focalplane\n",
    ")\n",
    "\n",
    "# Capture C++ output in the jupyter cells\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Environment\n",
    "\n",
    "You can get the current TOAST runtime configuration from the \"Environment\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast\n",
    "\n",
    "env = toast.Environment.get()\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Data Model\n",
    "\n",
    "Before using TOAST for simulation or analysis, it is important to discuss how data is stored in memory and how that data can be distributed among many processes to parallelize large workflows.\n",
    "\n",
    "First, let's create a fake focalplane of detectors to use throughout this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate a fake focalplane with 7 pixels, each with 2 detectors.\n",
    "\n",
    "fp = fake_focalplane()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of this focalplane layout.\n",
    "\n",
    "detnames = list(sorted(fp.keys()))\n",
    "detquat = {x: fp[x][\"quat\"] for x in detnames}\n",
    "detfwhm = {x: fp[x][\"fwhm_arcmin\"] for x in detnames}\n",
    "detlabels = {x: x for x in detnames}\n",
    "detpolcol = {x: \"red\" if i % 2 == 0 else \"blue\" for i, x in enumerate(detnames)}\n",
    "\n",
    "toast.tod.plot_focalplane(\n",
    "    detquat, 4.0, 4.0, None, fwhm=detfwhm, polcolor=detpolcol, labels=detlabels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations with Time Ordered Data\n",
    "\n",
    "TOAST works with data organized into *observations*.  Each observation is independent of any other observation.  An observation consists of co-sampled detectors for some span of time.  The intrinsic detector noise is assumed to be stationary within an observation.  Typically there are other quantities which are constant for an observation (e.g. elevation, weather conditions, satellite spin axis, etc).\n",
    "\n",
    "An observation is just a dictionary with at least one member (\"tod\") which is an instance of a class that derives from the `toast.TOD` base class.\n",
    "\n",
    "The inputs to a TOD class constructor are at least:\n",
    "\n",
    "1. The detector names for the observation.\n",
    "2. The number of samples in the observation.\n",
    "3. The geometric offset of the detectors from the boresight.\n",
    "4. Information about how detectors and samples are distributed among processes.  More on this below.\n",
    "\n",
    "The TOD class can act as a storage container for different \"flavors\" of timestreams as well as a source and sink for the observation data (with the read_\\*() and write_\\*() methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.qarray as qa\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "obs = dict()\n",
    "obs[\"name\"] = \"20191014_000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The type of TOD class is usually specific to the data processing job.\n",
    "# For example it might be one of the simulation classes or it might be\n",
    "# a class that loads experiment data.  Here we just use a simple class\n",
    "# that is only used for testing and which reads / writes data to internal memory\n",
    "# buffers.\n",
    "\n",
    "tod = toast.tod.TODCache(None, detnames, nsamples, detquats=detquat)\n",
    "obs[\"tod\"] = tod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tod to get summary info:\n",
    "print(tod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TOD class has methods to get information about the data:\n",
    "\n",
    "print(\"TOD has detectors {}\".format(\", \".join(tod.detectors)))\n",
    "print(\"TOD has {} total samples for each detector\".format(tod.total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some data.  Not every TOD derived class supports writing (for example,\n",
    "# TOD classes that represent simulations).\n",
    "\n",
    "def fill_tod(tod, fp):\n",
    "    detnames = tod.detectors\n",
    "    t_delta = 1.0 / fp[detnames[0]][\"rate\"]\n",
    "    tod.write_times(stamps=np.arange(0.0, nsamples * t_delta, t_delta))\n",
    "    tod.write_boresight(\n",
    "        data=qa.from_angles(\n",
    "            (np.pi / 2) * np.ones(nsamples),\n",
    "            (2 * np.pi / nsamples) * np.arange(nsamples),\n",
    "            np.zeros(nsamples)\n",
    "        )\n",
    "    )\n",
    "    tod.write_position(pos=np.zeros((nsamples, 3), dtype=np.float64))\n",
    "    tod.write_velocity(vel=np.zeros((nsamples, 3), dtype=np.float64))\n",
    "    tod.write_common_flags(flags=np.zeros(nsamples, dtype=np.uint8))\n",
    "    for d in detnames:\n",
    "        tod.write(\n",
    "            detector=d, data=np.random.normal(\n",
    "                scale=fp[d][\"NET\"], \n",
    "                size=nsamples\n",
    "            )\n",
    "        )\n",
    "        tod.write_flags(\n",
    "            detector=d, flags=np.zeros(nsamples, dtype=np.uint8)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_tod(tod, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read it back\n",
    "\n",
    "print(\"TOD timestamps = {} ...\".format(tod.read_times()[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOD boresight = \\n{} ...\".format(tod.read_boresight()[:5,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in detnames:\n",
    "    print(\"TOD detector {} = {} ...\".format(\n",
    "        d, tod.read(detector=d, n=5))\n",
    "    )\n",
    "    print(\"TOD detector {} flags = {} ...\".format(\n",
    "        d, tod.read_flags(detector=d, n=5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store some data in the cache.  The \"cache\" member variable looks like a dictionary of\n",
    "# numpy arrays, but the memory used is allocated in C, so that we can actually clear\n",
    "# these buffers when needed.\n",
    "\n",
    "for d in detnames:\n",
    "    processed = tod.read(detector=d)\n",
    "    processed /= 2.0\n",
    "    # By convention, we usually name buffers in the cache by <prefix>_<detector>\n",
    "    tod.cache.put(\"processed_{}\".format(d), processed)\n",
    "print(\"TOD cache now contains {} bytes\".format(tod.cache.report(silent=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common use pattern is to \"read and cache\" data.  This happens if we want to keep the data in memory to re-use later.  The TOD class has a set of methods that start with the string \"local_\" that perform this action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from cache or read and cache\n",
    "\n",
    "print(\"TOD timestamps = {} ...\".format(tod.local_times()[:2]))\n",
    "for d in detnames:\n",
    "    print(\"TOD detector {} = {} ...\".format(\n",
    "        d, tod.local_signal(d)[:2])\n",
    "    )\n",
    "    print(\"TOD detector {} pointing = {} ...\".format(\n",
    "        d, tod.local_pointing(d)[:2,:])\n",
    "    )\n",
    "    print(\"TOD detector {} flags = {} ...\".format(\n",
    "        d, tod.local_flags(d)[:2])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comm : Groups of Processes\n",
    "\n",
    "A toast.Comm instance takes the global number of processes available (MPI.COMM_WORLD) and divides them into groups.  Each process group is assigned one or more observations.  Since observations are independent, this means that different groups can be independently working on separate observations in parallel.  It also means that inter-process communication needed when working on a single observation can occur with a smaller set of processes.\n",
    "\n",
    "At NERSC, this notebook is running on a login node, so we cannot use MPI.  Constructing a default `toast.Comm` whenever MPI use is disabled will just produce a single group of one process.  See the parallel example at the end of this notebook for a case with multiple groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = toast.Comm()\n",
    "print(comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data : a Collection of Observations\n",
    "\n",
    "A toast.Data instance is mainly just a list of observations.  However remember that each process group will have a different set of observations.  Since we have only one group of one process, this example is not so interesting.  See the MPI example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data(comm)\n",
    "data.obs.append(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Recapping previous sections, we have some groups of processes, each of which has a set of observations.  Within a single process group, the detector data is distributed across the processes within the group.  That distribution is controlled by the size of the communicator passed to the TOD class, and also by the `detranks` parameter of the constructor.  This detranks number sets the dimension of the process grid in the detector direction.  For example, a value of \"1\" means that every process has all detectors for some span of time.  A value equal to the size of the communicator results in every process having some number of detectors for the entire observation.  The detranks parameter must divide evenly into the number of processes in the communicator and determines how the processes are arranged in a grid.\n",
    "\n",
    "As a concrete example, imagine that MPI.COMM_WORLD has 4 processes.  We split this into 2 groups of 2 procesess.  There are 3 observations of varying lengths and every group has one or 2 observations. Here is the starting point of our data distribution:\n",
    "<img src=\"data_dist_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the processes into 2 groups\n",
    "<img src=\"data_dist_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we assign our observations to the two groups\n",
    "<img src=\"data_dist_3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create the TOD class in each observation, we specify how the the data is distributed within each observation.  If the `detranks` parameter is \"1\", then the dimension of the process grid in the detector direction is one.\n",
    "<img src=\"data_dist_4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `detranks` is set to the size of the group, then we get:\n",
    "<img src=\"data_dist_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Data\n",
    "\n",
    "Once we have our distributed data set up, we usually feed this through a `pipeline`.  There will be a lesson on pipelines later.  Here we will create an entire fake dataset and work with it.  The MPI introduction notebook will go into more details about working with distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = toast.Comm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data(comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 observations, each containing an TODCache class.  We'll\n",
    "# use the same focalplane and number of samples for each observation,\n",
    "# but this is not required- each observation is independent.\n",
    "\n",
    "for i in range(3):\n",
    "    obsname = \"observation_{:02d}\".format(i)\n",
    "    obs = dict()\n",
    "    obs[\"name\"] = obsname\n",
    "    obs[\"id\"] = \"{:02d}\".format(i)\n",
    "    obs[\"tod\"] = toast.tod.TODCache(\n",
    "        comm.comm_group, detnames, nsamples, detquats=detquat\n",
    "    )\n",
    "    fill_tod(obs[\"tod\"], fp)\n",
    "    data.obs.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does our distributed data look like now?\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a dump of the distributed data, and in this case the `TODCache` class is storing stuff \"under the hood\", which is why data shows up in the dump of the cache as well as when calling the normal TOD access methods.\n",
    "\n",
    "Next, we can dump this data to a TIDAS volume (directories of HDF5 files in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if toast.tod.tidas_available:\n",
    "    import toast.tod.tidas as tt\n",
    "    datapath = \"intro_data\"\n",
    "    if os.path.isdir(datapath):\n",
    "        shutil.rmtree(datapath)\n",
    "    exporter = tt.OpTidasExport(\n",
    "        datapath,\n",
    "        tt.TODTidas,\n",
    "        backend=\"hdf5\",\n",
    "        comp=\"none\",\n",
    "        use_todchunks=True,\n",
    "    )\n",
    "    exporter.exec(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can load it back in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toast.tod.tidas_available:\n",
    "    data = tt.load_tidas(\n",
    "        comm,\n",
    "        comm.group_size,\n",
    "        datapath,\n",
    "        \"r\",\n",
    "        \"detectors\",\n",
    "        tt.TODTidas,\n",
    "        distintervals=\"chunks\",\n",
    "        group_dets=\"detectors\",\n",
    "    )\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Utilities\n",
    "\n",
    "There are many utilities in the TOAST package that use compiled code internally.  These include:\n",
    "\n",
    "- `toast.rng`:  Streamed random number generation, with support for generating random samples from any location within a stream.\n",
    "\n",
    "- `toast.qarray`:  Vectorized quaternion operations.\n",
    "\n",
    "- `toast.fft`:  API Wrapper around different vendor FFT packages.\n",
    "\n",
    "- `toast.cache`:  Class for dictionary of C-allocated numpy arrays.\n",
    "\n",
    "- `toast.healpix`:  Subset of pixel projection routines, simd vectorized and threaded.\n",
    "\n",
    "- `toast.timing`:  Simple serial timers, global named timers per process, a decorator to time calls to functions, and MPI tools to gather timing statistics from multiple processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Number Example\n",
    "\n",
    "Here is a quick example of a threaded generation of random numbers drawn from a unit-variance gaussian distribution.  Note the \"key\" pair of uint64 values and the first value of the \"counter\" pair determine the stream, and the second value of the counter pair is effectively the sample in that stream.  We can drawn randoms from anywhere in the stream in a reproducible fashion (i.e. this random generator is stateless).  Under the hood, this uses the Random123 package on each thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.rng as rng\n",
    "\n",
    "# Number of random samples\n",
    "nrng = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw randoms from the beginning of a stream\n",
    "rng1 = rng.random(\n",
    "    nrng, key=[12, 34], counter=[56, 0], sampler=\"gaussian\", threads=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw randoms from some later starting point in the stream\n",
    "rng2 = rng.random(\n",
    "    nrng, key=[12, 34], counter=[56, 4], sampler=\"gaussian\", threads=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned objects are buffer providers, so can be used like a numpy array.\n",
    "print(\"Returned RNG buffers:\")\n",
    "print(rng1)\n",
    "print(rng2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the elements.  Note how the overlapping sample indices match.  The\n",
    "# randoms drawn for any given sample agree regardless of the starting sample.\n",
    "print(\"------ rng1 ------\")\n",
    "for i in range(nrng):\n",
    "    print(\"rng1 {}:  {}\".format(i, rng1[i]))\n",
    "print(\"------ rng2 ------\")\n",
    "for i in range(nrng):\n",
    "    print(\"rng2 {}:  {}\".format(i + 4, rng2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quaternion Array Example\n",
    "\n",
    "The quaternion manipulation functions internally attempt to improve performance using OpenMP SIMD directives and threading in cases where it makes sense.  The Python API is modelled after the quaternionarray package (https://github.com/zonca/quaternionarray/).  There are functions for common operations like multiplying quaternion arrays, rotating arrays of vectors, converting to and from angle representations, SLERP, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.qarray as qa\n",
    "\n",
    "# Number points for this example\n",
    "\n",
    "nqa = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake rotation data by sweeping through theta / phi / pa angles\n",
    "\n",
    "theta = np.linspace(0.0, np.pi, num=nqa)\n",
    "phi = np.linspace(0.0, 2 * np.pi, num=nqa)\n",
    "pa = np.zeros(nqa)\n",
    "print(\"----- input angles -----\")\n",
    "print(\"theta = \", theta)\n",
    "print(\"phi = \", phi)\n",
    "print(\"pa = \", pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to quaternions\n",
    "\n",
    "quat = qa.from_angles(theta, phi, pa)\n",
    "\n",
    "print(\"\\n----- output quaternions -----\")\n",
    "print(quat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these to rotate a vector\n",
    "\n",
    "zaxis = np.array([0.0, 0.0, 1.0])\n",
    "zrot = qa.rotate(quat, zaxis)\n",
    "\n",
    "print(\"\\n---- Z-axis rotated by quaternions ----\")\n",
    "print(zrot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate different vector by each quaternion\n",
    "\n",
    "zout = qa.rotate(quat, zrot)\n",
    "\n",
    "print(\"\\n---- Arbitrary vectors rotated by quaternions ----\")\n",
    "print(zout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply two quaternion arrays\n",
    "\n",
    "qcopy = np.array(quat)\n",
    "qout = qa.mult(quat, qcopy)\n",
    "\n",
    "print(\"\\n---- Product of two quaternion arrays ----\")\n",
    "print(qout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLERP quaternions\n",
    "\n",
    "qtime = 3.0 * np.arange(nqa)\n",
    "qtargettime = np.arange(3.0 * (nqa - 1) + 1)\n",
    "qslerped = qa.slerp(qtargettime, qtime, quat)\n",
    "\n",
    "print(\"\\n---- SLERP input ----\")\n",
    "for t, q in zip(qtime, quat):\n",
    "    print(\"t = {} : {}\".format(t, q))\n",
    "    \n",
    "print(\"\\n---- SLERP output ----\")\n",
    "for t, q in zip(qtargettime, qslerped):\n",
    "    print(\"t = {} : {}\".format(t, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT Example\n",
    "\n",
    "The internal FFT functions in TOAST are very limited and focus only on batched 1D Real FFTs.  These are used for simulated noise generation and timestream filtering.  Internally the compiled code can use either FFTW or MKL for the backend calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batched FFTs\n",
    "\n",
    "nbatch = 5\n",
    "\n",
    "# FFT length\n",
    "\n",
    "nfft = 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake data\n",
    "\n",
    "infft = np.zeros((nbatch, nfft), dtype=np.float64)\n",
    "for b in range(nbatch):\n",
    "    infft[b, :] = rng.random(nfft, key=[0, 0], counter=[b, 0], sampler=\"gaussian\")\n",
    "\n",
    "print(\"----- FFT input -----\")\n",
    "print(infft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward FFT\n",
    "\n",
    "outfft = toast.fft.r1d_forward(infft)\n",
    "\n",
    "print(\"\\n----- FFT output -----\")\n",
    "print(outfft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse FFT\n",
    "\n",
    "backfft = toast.fft.r1d_backward(outfft)\n",
    "\n",
    "print(\"\\n----- FFT inverse output -----\")\n",
    "print(backfft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Example\n",
    "\n",
    "The Cache class provides a mechanism to work around the Python memory pool.  There are times when we want to allocate memory and explicitly free it without waiting for garbage collection.  Every instance of a `toast.Cache` acts as a dictionary of numpy arrays.  Internally, the memory of each entry is a flat-packed std::vector with a custom allocator that ensures aligned memory allocation.  Aligned memory is required for SIMD operations both in TOAST and in external libraries.  Buffers in a Cache instance can be used directly for such operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toast.cache import Cache\n",
    "\n",
    "# Example array dimensions\n",
    "\n",
    "cnames = [\"c1\", \"c2\"]\n",
    "cshapes = {\n",
    "    \"c1\" : (20,),\n",
    "    \"c2\" : (2, 3, 2)\n",
    "}\n",
    "ctyps = {\n",
    "    \"c1\" : np.float64,\n",
    "    \"c2\" : np.uint16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cache instance\n",
    "\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some empty arrays in the cache\n",
    "\n",
    "for cn in cnames:\n",
    "    cache.create(cn, ctyps[cn], cshapes[cn])\n",
    "\n",
    "print(\"---- Cache object ----\")\n",
    "print(cache)\n",
    "print(\"\\n---- Now contains ----\")\n",
    "for cn in cnames:\n",
    "    print(\"{}:  {}\".format(cn, cache.reference(cn)))\n",
    "print(\"Size = \", cache.report(silent=True), \" bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill existing buffers\n",
    "\n",
    "# Get a reference to the buffer\n",
    "cdata = cache.reference(\"c1\")\n",
    "\n",
    "# Assign elements.\n",
    "cdata[:] = np.random.random(cshapes[\"c1\"])\n",
    "\n",
    "# Delete the reference\n",
    "del cdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = cache.reference(\"c2\")\n",
    "idx = 0\n",
    "for x in range(cshapes[\"c2\"][0]):\n",
    "    for y in range(cshapes[\"c2\"][1]):\n",
    "        for z in range(cshapes[\"c2\"][2]):\n",
    "            cdata[x, y, z] = idx\n",
    "            idx += 1\n",
    "del cdata\n",
    "    \n",
    "print(\"\\n---- Contents after filling ----\")\n",
    "for cn in cnames:\n",
    "    print(\"{}:  {}\".format(cn, cache.reference(cn)))\n",
    "print(\"Size = \", cache.report(silent=True), \" bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also \"put\" existing numpy arrays which will then be copied into\n",
    "# the cache\n",
    "\n",
    "np1 = np.random.normal(size=10)\n",
    "np2 = np.random.randint(0, high=255, dtype=np.uint16, size=12).reshape((2, 3, 2))\n",
    "\n",
    "cache.put(\"p1\", np1)\n",
    "cache.put(\"p2\", np2)\n",
    "\n",
    "print(\"\\n---- Contents after putting numpy arrays ----\")\n",
    "\n",
    "for cn in list(cache.keys()):\n",
    "    print(\"{}:  {}\".format(cn, cache.reference(cn)))\n",
    "print(\"Size = \", cache.report(silent=True), \" bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Test Suite\n",
    "\n",
    "TOAST includes extensive tests built in to the package.  Running all of them takes some time, but you can also run just one test by specifying the name of the file in the toast/tests directory (without the \".py\" extension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.tests\n",
    "\n",
    "# Run just a couple simple tests in toast/tests/env.py\n",
    "toast.tests.run(\"env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run **ALL** the (serial) tests\n",
    "# toast.tests.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
