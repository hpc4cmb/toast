{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This lesson is a brief introduction to TOAST:  how data is represented in memory and how to build processing workflows.  First we import some packages we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# External modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "\n",
    "# TOAST\n",
    "import toast\n",
    "\n",
    "\n",
    "# Capture C++ output in the jupyter cells\n",
    "%load_ext wurlitzer\n",
    "\n",
    "# Display inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Environment\n",
    "\n",
    "The `toast` module can be influenced by a few environment variables, which must be set **before** importing `toast`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(toast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the current TOAST runtime configuration from the \"Environment\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = toast.Environment.get()\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logging level can be changed by either setting the `TOAST_LOGLEVEL` environment variable to one of the supported levels (`VERBOSE`, `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) or by using the `set_log_level()` method of the `Environment` class.  The maximum number of threads is controlled by the standard `OMP_NUM_THREADS` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Data Model\n",
    "\n",
    "The basic data model in a toast workflow consists of a set of `Observation` instances, each of which is associated with a `Focalplane` on a `Telescope`.  Note that a Focalplane instance is probably just a sub-set of detectors on the actual physical focalplane.  These detectors must be co-sampled and likely have other things in common (for example, they are on the same wafer or are correlated in some other way).  For this notebook, we will manually create these objects, but usually these will be loaded / created by some experiment-specific function.\n",
    "\n",
    "MPI is completely optional in TOAST, although it is required to achieve good parallel performance on systems with many (e.g. 4 or more) cores.  Most of the parallelism in TOAST is MPI process-based, not threaded.  In this section we show how interactive use of TOAST can be done without any reference to MPI.  In a separate notebook in this directory we show how to make use of distributed data and operations in parallel workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by making a fake focalplane\n",
    "\n",
    "from toast.instrument_sim import (\n",
    "    fake_hexagon_focalplane,\n",
    "    plot_focalplane,\n",
    ")\n",
    "\n",
    "focalplane_pixels = 7 # (hexagonal, pixel zero at center)\n",
    "field_of_view = 10.0 * u.degree\n",
    "sample_rate = 10.0 * u.Hz\n",
    "\n",
    "focalplane = fake_hexagon_focalplane(\n",
    "    n_pix=focalplane_pixels,\n",
    "    width=field_of_view,\n",
    "    sample_rate=sample_rate,\n",
    "    epsilon=0.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of this focalplane layout.\n",
    "\n",
    "detpolcol = {\n",
    "    x: \"red\" if x.endswith(\"A\") else \"blue\" for x in focalplane.detectors\n",
    "}\n",
    "\n",
    "plot_focalplane(\n",
    "    focalplane=focalplane,\n",
    "    width=1.3 * field_of_view,\n",
    "    height=1.3 * field_of_view,\n",
    "    show_labels=True,\n",
    "    pol_color=detpolcol\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make a fake telescope\n",
    "\n",
    "telescope = toast.Telescope(name=\"fake\", focalplane=focalplane, site=toast.SpaceSite(name=\"L2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a fake telescope created, we can create an observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty observation\n",
    "\n",
    "samples = 10\n",
    "\n",
    "ob = toast.Observation(\n",
    "    telescope, \n",
    "    name=\"2020-07-31_A\", \n",
    "    n_samples=samples\n",
    ")\n",
    "\n",
    "print(ob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see our observation simply has the starting information we passed to the constructor.  Next we will discuss the 3 types of data objects that can be stored in an Observation:  detector data products, shared telescope data, and arbitrary metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "By default, the observation is empty.  You can add arbitrary metadata to the observation- it acts just like a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = {\n",
    "    \"Temperature 1\": np.array([1.0, 2.0, 3.0]),\n",
    "    \"Other Sensor\": 1.2345\n",
    "}\n",
    "\n",
    "ob[\"housekeeping\"] = hk\n",
    "\n",
    "print(ob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata like this is not synchronized in any way between processes.  A user or Operator can put any keys here to store small data objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector Data\n",
    "\n",
    "Detector data has some unique properties that we often want to leverage in our analyses. Each process has some detectors and some time slice of the observation. In the case of a single process like this example, all the data is local. Before using data we need to create it within the empty Observation. Here we create a \"signal\" object for the detectors.  The detector data is accessed under the `detdata` attribute of the observation:\n",
    "\n",
    "\n",
    "**FIXME:  talk about naming conventions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create and initialize to zero some detector data named \"signal\".  This has one value per sample per detector and each value is a 64bit float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.detdata.create(\"signal\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ob.detdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ob.detdata[\"signal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create other types of detector data, and there is some shortcut notation that can be used to create detector data objects from existing arrays.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes an existing N_detector x N_sample array and creates from that\n",
    "\n",
    "some_data = 3.0 * np.ones(\n",
    "    (\n",
    "        len(ob.local_detectors), \n",
    "        ob.n_local_samples\n",
    "    ),\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "ob.detdata[\"existing_signal\"] = some_data\n",
    "print(ob.detdata[\"existing_signal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes one detectors-worth of data and replicates it to all detectors\n",
    "# while creating a new data object.\n",
    "\n",
    "ob.detdata[\"replicated\"] = 5 * np.ones(ob.n_local_samples, dtype=np.int32)\n",
    "print(ob.detdata[\"replicated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create detector data objects from a dictionary\n",
    "# of single-detector arrays\n",
    "other = dict()\n",
    "for i, d in enumerate(ob.local_detectors):\n",
    "    other[d] = i * np.ones(ob.n_local_samples, dtype=np.int32)\n",
    "\n",
    "ob.detdata[\"other_signal\"] = other\n",
    "print(ob.detdata[\"other_signal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default you will get detector data with one element per sample and float64 dtype.  However, you can specify the shape of each detector sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of data with different shape\n",
    "\n",
    "ob.detdata.create(\"pointing\", sample_shape=(4,), dtype=np.float32)\n",
    "print(ob.detdata[\"pointing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of Detector Data\n",
    "\n",
    "In the commands above we created named data objects and each one seems to contain an array for each detector.  However, this container actually allocates memory in a single block, and you can slice the object both in the detector and sample direction.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access one detector by name\n",
    "ob.detdata[\"signal\"][\"D0A\"] = np.arange(samples, dtype=np.float64)\n",
    "\n",
    "# Access one detector by index\n",
    "ob.detdata[\"signal\"][1] = 10.0 * np.arange(samples, dtype=np.float64)\n",
    "\n",
    "# Slice by both detector and sample\n",
    "ob.detdata[\"signal\"][[\"D2A\", \"D2B\"], 0:2] = 5.0\n",
    "\n",
    "print(ob.detdata[\"signal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the whole thing as a 2D array\n",
    "print(ob.detdata[\"signal\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Data\n",
    "\n",
    "Many types of data are common to multiple detectors.  Some examples would be telescope pointing, timestamps, other sensor data, etc.  When running in parallel we want to have just one copy of this data per node in order to save memory.  The shared data is accessed under the \"shared\" attribute of the observation.  For this serial notebook, you will not need to worry about the details of communicators, but when running in parallel it becomes important.  \n",
    "For this serial notebook, the `shared` attribute will look very much like a dictionary of numpy arrays.  See the \"parallel\" intro notebook for more examples of using shared data when each observation is distributed across a grid of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some time stamps by assigning from an existing array on one process.\n",
    "# When running with multiple processes, this syntax has extra communication.\n",
    "ob.shared[\"times\"] = np.arange(ob.n_local_samples, dtype=np.float64)\n",
    "print(ob.shared[\"times\"])\n",
    "\n",
    "# Create and initialize to zero some boresight quaternions\n",
    "ob.shared.create(\"boresight_radec\", shape=(ob.n_local_samples, 4), dtype=np.float64)\n",
    "print(ob.shared[\"boresight_radec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data objects are a special \"MPIShared\" object from the [`pshmem`](https://pypi.org/project/pshmem/) package.  Shared data objects can be read with slicing notation just like normal numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ob.shared[\"boresight_radec\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, they are intended to be \"write once\", \"read many\" objects.  You cannot simply assign data to them.  The reason is that the data is replicated across nodes and so setting array values must be a collective operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullquat = np.array([0.0, 0.0, 0.0, 1.0])\n",
    "full_data = np.tile(nullquat, ob.n_local_samples).reshape((-1, 4))\n",
    "\n",
    "# In the serial case, simple assignment works just like array assignment\n",
    "ob.shared[\"boresight_radec\"][:] = full_data\n",
    "\n",
    "# When running with MPI, the set() method avoids some communication\n",
    "ob.shared[\"boresight_radec\"].set(full_data, fromrank=0)\n",
    "\n",
    "print(ob.shared[\"boresight_radec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervals\n",
    "\n",
    "Each `Observation` may contain one or more \"interval lists\" which act as a global (within the observation) list of time / sample ranges where some feature of the data is constant.  Interval lists support sample-wise inversion, intersection and union operations using the standard python bitwise operators (`^`, `&`, and `|`).\n",
    "\n",
    "Intervals are **not** intended to act as individual sample quality flags.  Per-sample flags should be created either as a shared timestream (for flags common to all detectors) or as a detector data object (for per-detector flags).  Intervals can be used to represent things changing less frequently, for example:  left or right moving telescope scans, satellite repointing maneuvers, calibration measurements, etc.\n",
    "\n",
    "A single `Interval` consists of a time and a (local) sample range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? toast.Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observation starts with no lists of intervals\n",
    "\n",
    "ob.intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a new interval list, use the `create()` method.  Remember, in this notebook we have only one process, so do not have to worry about which process this information is coming from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ob.intervals.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create one list of intervals.  We specify the time ranges and the local array of timestamp values.  Inside the code, the timestamps are used to convert these input time ranges into `Interval` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.intervals.create(\"good\", [(1.5, 3.5), (4.5, 6.), (7., 8.5)], ob.shared[\"times\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there is one interval list in the observation\n",
    "\n",
    "print(ob.intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The create method converted the time ranges into actual Interval instances:\n",
    "\n",
    "print(ob.intervals[\"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create another list of intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.intervals.create(\"stable\", [(0.5, 2.5), (3.5, 5.), (6., 7.5)], ob.shared[\"times\"])\n",
    "print(ob.intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, we can combine these in different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.intervals[\"stable-and-not-good\"] = ob.intervals[\"stable\"] & ~ob.intervals[\"good\"]\n",
    "\n",
    "print(ob.intervals)\n",
    "print(ob.intervals[\"stable-and-not-good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.intervals[\"not-stable-or-not-good\"] = ~ob.intervals[\"stable\"] | ~ob.intervals[\"good\"]\n",
    "\n",
    "print(ob.intervals)\n",
    "print(ob.intervals[\"not-stable-or-not-good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views\n",
    "\n",
    "Typically when defining data intervals in the last section it is because you want to do something with only the data falling in those sample ranges.  Each observation has the ability to provide a \"view\" into the detector and shared data given by a previously defined interval list.  Views are created on the fly on first access and are deleted automatically if the underlying interval is deleted.  First, examine a view of the \"good\" interval list we defined in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ob.view[\"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string represention of a view is just a list of sample slices.  However, the real power is that we can get a view of any of the observation `detdata` or `shared` objects.  For example, we could get a view of the detector `signal` data.  Recall that the full data for this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.detdata[\"signal\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A view of the signal data falling in the \"good\" intervals is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.view[\"good\"].detdata[\"signal\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This view is a list of arrays which have sliced the data in the time direction.  These are **not** copies- they provide read/write access to underlying buffer.  If you are doing many operations with a view it is easier to name it something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sng = ob.view[\"stable-and-not-good\"]\n",
    "sng.detdata[\"signal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use a view to assign data to a subset of the full samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sng.detdata[\"signal\"] = 7.0\n",
    "\n",
    "print(ob.detdata[\"signal\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access shared data as well with this view, but it is read-only from the view (the `set()` method of the shared objects or a collective assignment must be used to modify shared data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.view[\"good\"].shared[\"boresight_radec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sng.shared[\"boresight_radec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Container\n",
    "\n",
    "The `Observation` instances discussed previously are usually stored as a list inside a top-level container class called `Data`.  This class also stores the TOAST MPI communicator information.  For this serial example you can just instantiate an empty `Data` class and add things to the observation list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data()\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(data.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this `Data` object has no observations yet.  We'll fix that in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Model\n",
    "\n",
    "The TOAST processing model consists of `Operator` class instances running in a sequence on a subset of data.  These sequences could be nested within other sequences (see the `Pipeline` operator below).\n",
    "\n",
    "The Operator base class defines the interfaces for operators working on data.  Operators are configured by defining class traits (attributes) which can be set during construction.  An operator has an `exec()` method that works with Data objects (potentially just a subset of the data).  Operators also have a `finalize()` method which is designed to do any final calculations after all passes through the timestream data are done. We will start by looking at the SimSatellite operator to simulate fake telescope scan strategies for a generic satellite. We can always see the options and default values by using the standard help function or the '?' command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toast import ops\n",
    "\n",
    "?ops.SimSatellite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instantiate a class directly by overriding some defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsat = ops.SimSatellite(\n",
    "    num_observations=2, \n",
    "    observation_time=5 * u.minute,\n",
    ")\n",
    "\n",
    "print(simsat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using multi instances of an operator in your pipeline with different configurations, then you should also pass a unique \"name\" to the constructor.  This allows keeping the operators distinct when using config files (see more below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_simsat = ops.SimSatellite(\n",
    "    name=\"other_simsat\",\n",
    "    num_observations=2, \n",
    "    observation_time=5 * u.minute,\n",
    ")\n",
    "\n",
    "print(other_simsat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the operator is constructed, the parameters can be changed directly. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsat.telescope = telescope\n",
    "simsat.num_observations = 3\n",
    "\n",
    "print(simsat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have an `Operator` that is ready to use.  This particular operator creates observations from scratch with telescope properties generated and stored.  We can create an empty `Data` object and then run this operator on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is equivalent to single call to \"exec()\" with all processes,\n",
    "# and then a call to \"finalize()\".\n",
    "\n",
    "simsat.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this trivial case, we use the `apply()` method of the operator, which simply calls `exec()` once and then `finalize()`.  When running a more complicated pipeline, the `exec()` method might be called multiple times on different detector sets (for example) before calling `finalize()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "TOAST includes a special operator (the `Pipeline` class), which is designed to run other operators (including other Pipeline instances.  The purpose of this operator is to run sequences of other operators over sets of detectors to reduce the memory cost of intermediate products and / or to group together operators that support the use of accelerators to avoid memory copies to the host system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? ops.Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can create two simple operators and put them in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsat = ops.SimSatellite(\n",
    "    num_observations=2, \n",
    "    observation_time=5 * u.minute,\n",
    "    telescope=telescope\n",
    ")\n",
    "\n",
    "default_noise = ops.DefaultNoiseModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ops.Pipeline(\n",
    "    operators=[simsat, default_noise]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with an empty Data object and run the pipeline on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = toast.Data()\n",
    "\n",
    "pipe.apply(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the same satellite simulation was run, and then a default noise model (using the focalplane properties in each observation) was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of Operators\n",
    "\n",
    "Operators are configured through class traits which can be passed as keyword arguments to the constructor.  We can also dump information about these traits (name, type, help string) to an intermediate config dictionary and then write that to files in TOML or JSON format.  These config dictionaries can also be used to instantiate operators directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.config as tc\n",
    "\n",
    "import tempfile\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "pp = PrettyPrinter(indent=1)\n",
    "\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "toml_file = os.path.join(tmpdir, \"test.toml\")\n",
    "json_file = os.path.join(tmpdir, \"test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can take a previous operator and look at the \"round trip\" from class or instance, to a config dictionary, to a file, and back into creating a new operator instance from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the config for an existing instance\n",
    "\n",
    "conf = other_simsat.get_config()\n",
    "pp.pprint(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the default config values for a class\n",
    "\n",
    "default_conf = ops.SimSatellite.get_class_config()\n",
    "pp.pprint(default_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.dump_toml(toml_file, conf)\n",
    "tc.dump_json(json_file, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what this config looks like dumped to TOML and JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {toml_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {json_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can load the config back in to a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newconf = tc.load_config(toml_file)\n",
    "pp.pprint(newconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create new instances of operators from this config dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = tc.create(newconf)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we access our new operator and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_simsat = run[\"operators\"][\"other_simsat\"]\n",
    "print(new_simsat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Test Suite\n",
    "\n",
    "TOAST includes extensive tests built in to the package.  Running all of them takes some time, but you can also run just one test by specifying the name of the file in the toast/tests directory (without the \".py\" extension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toast.tests\n",
    "\n",
    "# Run just a couple simple tests in toast/tests/env.py\n",
    "toast.tests.run(\"env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run **ALL** the (serial) tests\n",
    "# toast.tests.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
